{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4efab641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added '/home/brimmann/works/xRAG' to sys.path\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Assumes the notebook is in src/distill.\n",
    "# Go up two levels to the project root.\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "print(f\"Added '{project_root}' to sys.path\")\n",
    "\n",
    "# Now you can import from src.model\n",
    "# For example:\n",
    "# from src.model.your_file import your_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "918a763a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "116fb57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt_with_mistral_chat_format(messages,tokenizer,*args,**kwargs):\n",
    "    # return tokenizer.apply_chat_template(messages,tokenize=False,add_special_tokens=False)\n",
    "    formatted_text = \"\"\n",
    "    for message in messages:\n",
    "        if message['role'] == 'user':\n",
    "            formatted_text += \"[INST] \" + message['content'] + \" [/INST]\"\n",
    "        elif message['role'] == 'assistant':\n",
    "            formatted_text += message['content'] + tokenizer.eos_token\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Mistral chat template only supports 'user' and 'assistant' roles. Invalid role: {}.\".format(message[\"role\"])\n",
    "                )\n",
    "    # formatted_text += \" The answer is:\"\n",
    "    return formatted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5a8523",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    args = argparse.Namespace(\n",
    "        # --- Set your desired default values here ---\n",
    "        retrieval_prefix='colbertv2',\n",
    "        tf_idf_topk=0,\n",
    "        base_model=None,  # e.g., 'path/to/base_model'\n",
    "        use_rag=True,  # This will be set to True if retriever_name_or_path is provided\n",
    "        enable_progress_bar=True,\n",
    "        data='triviaqa',  # e.g., 'nq_open', 'hotpotqa', 'triviaqa', 'webqa', 'truthfulqa', 'factkg'\n",
    "        model_name_or_path='Hannibal046/xrag-7b',  # e.g., 'path/to/your/model'\n",
    "        eval_metrics=None,  # This is set based on the 'data' argument below\n",
    "        n_shot=0,\n",
    "        retriever_name_or_path='Salesforce/SFR-Embedding-Mistral',  # e.g., 'colbertv2/colbertv2.0'\n",
    "        retrieval_topk=[1],\n",
    "        retrieval_embed_length=0,\n",
    "        max_test_samples=4,  # e.g., 100 for debugging\n",
    "        save_dir='./outputs',  # e.g., 'path/to/save/results'\n",
    "        eval_batch_size=4,\n",
    "        chat_format='mistral',\n",
    "    )\n",
    "\n",
    "    ## post-process\n",
    "    if args.data in ['nq_open','hotpotqa','triviaqa','webqa']:\n",
    "        args.task_type = 'open_qa'\n",
    "        args.eval_metrics = 'substring_match'\n",
    "    elif args.data in ['truthfulqa']:\n",
    "        args.task_type = 'open_qa'\n",
    "        args.eval_metrics = 'truthfulqa_f1_rl'\n",
    "    elif args.data in ['factkg']:\n",
    "        args.task_type = 'fact_checking'\n",
    "        args.eval_metrics = 'fact_checking_acc'\n",
    "    \n",
    "    args.retrieval_topk = [x-1 for x in args.retrieval_topk] ## rank starts from 1\n",
    "    \n",
    "    if args.chat_format is not None:\n",
    "        args.chat_format = eval(f\"create_prompt_with_{args.chat_format}_chat_format\")    \n",
    "    \n",
    "    if args.retriever_name_or_path is not None:\n",
    "        args.use_rag = True\n",
    "\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e50610c",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a760005f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.retrieval_topk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1fc5e1b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Salesforce/SFR-Embedding-Mistral'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.retriever_name_or_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a08042a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brimmann/works/xRAG/.venv/lib/python3.9/site-packages/torch/cuda/__init__.py:51: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84bda8cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brimmann/works/xRAG/.venv/lib/python3.9/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    args.model_name_or_path,\n",
    "    padding_side = 'left',\n",
    "    add_eos_token=False, ## import to include this!\n",
    "    use_fast=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07460a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if tokenizer.pad_token:\n",
    "    pass\n",
    "elif tokenizer.unk_token:\n",
    "    tokenizer.pad_token_id = tokenizer.unk_token_id\n",
    "elif tokenizer.eos_token:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f42fd40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc14227d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "retrieval_embed_length = 0\n",
    "retriever,retriever_tokenizer = None,None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec78ad20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model import (\n",
    "    XMistralForCausalLM,\n",
    "    XMixtralForCausalLM,\n",
    "    SFR,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf7c4e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salesforce/SFR-Embedding-Mistral\n"
     ]
    }
   ],
   "source": [
    "print(args.retriever_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a006a8aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brimmann/works/xRAG/.venv/lib/python3.9/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af3a150dcfd84fc6a626ff2ff254de1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if args.retriever_name_or_path is not None:\n",
    "    \n",
    "    if args.retriever_name_or_path.lower() == 'salesforce/sfr-embedding-mistral':\n",
    "        retriever = SFR.from_pretrained(args.retriever_name_or_path,torch_dtype = torch.bfloat16)\n",
    "        retriever_tokenizer = AutoTokenizer.from_pretrained(args.retriever_name_or_path)\n",
    "    retrieval_embed_length = retriever.get_embed_length()\n",
    "    retriever_hidden_size = retriever.get_embed_dim()\n",
    "    retriever.eval()\n",
    "    retriever = retriever.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8abbf7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.eval.run_eval import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "176dd7cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/brimmann/works/xRAG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brimmann/works/xRAG/.venv/lib/python3.9/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c5766fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dockerfile  \u001b[0m\u001b[01;34mconfig\u001b[0m/  \u001b[01;34moffload\u001b[0m/            pyproject.toml.local  tutorial.ipynb\n",
      "README.md   \u001b[01;34mdata\u001b[0m/    prepare_data.ipynb  \u001b[01;34mscripts\u001b[0m/              uv.lock\n",
      "\u001b[01;34massets\u001b[0m/     main.py  pyproject.toml      \u001b[01;34msrc\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b4cac3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_data,test_data = load_dataset(\n",
    "    args.data,\n",
    "    args.use_rag,\n",
    "    args,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5ef4014a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '0',\n",
       " 'question': 'Who was the man behind The Chipmunks?',\n",
       " 'answer': ['David Seville'],\n",
       " 'entity': 'David Seville',\n",
       " 'background': ['Alvin and the Chipmunks | \" Alvin and the Chipmunks, originally David Seville and the Chipmunks or simply The Chipmunks, are an American animated virtual band created by Ross Bagdasarian for a novelty record in 1958. The group consists of three singing animated anthropomorphic chipmunks named Alvin, Simon, and Theodore. They are managed by their human adoptive father, David \"\"Dave\"\" Seville. Bagdasarian provided the group\\'s voices sped up to create high-pitched squeaky voices (which wasn\\'t entirely new to him, having worked on \"\"Witch Doctor\"\" earned the record two Grammy Awards for engineering). \"\"The Chipmunk Song\"\" became a number-one single in the United States. After Bagdasarian died in 1972, the characters’ voices were provided by his son Ross Bagdasarian Jr. and the latter\\'s wife Janice Karman in the subsequent incarnations of \"']}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "41bb7be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.max_test_samples is not None:\n",
    "    test_data = test_data[:args.max_test_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696ca008",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "916e719e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.eval.run_eval import prepare_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e749fbf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************** show one example ****************************************\n",
      "[INST] Refer to the background document and answer the questions:\n",
      "\n",
      "Background: <xRAG>\n",
      "\n",
      "Question: Who was the man behind The Chipmunks?? [/INST] The answer is:\n",
      "**************************************** show one example ****************************************\n"
     ]
    }
   ],
   "source": [
    "prompts,backgrounds = prepare_prompts(\n",
    "    dev_data = dev_data,\n",
    "    test_data = test_data,\n",
    "    task_type = args.task_type,\n",
    "    tokenizer = tokenizer,\n",
    "    n_shot = args.n_shot,\n",
    "    use_rag = args.use_rag,\n",
    "    retrieval_embed_length = retrieval_embed_length,\n",
    "    chat_format = args.chat_format, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dd71a610",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Alvin and the Chipmunks | \" Alvin and the Chipmunks, originally David Seville and the Chipmunks or simply The Chipmunks, are an American animated virtual band created by Ross Bagdasarian for a novelty record in 1958. The group consists of three singing animated anthropomorphic chipmunks named Alvin, Simon, and Theodore. They are managed by their human adoptive father, David \"\"Dave\"\" Seville. Bagdasarian provided the group\\'s voices sped up to create high-pitched squeaky voices (which wasn\\'t entirely new to him, having worked on \"\"Witch Doctor\"\" earned the record two Grammy Awards for engineering). \"\"The Chipmunk Song\"\" became a number-one single in the United States. After Bagdasarian died in 1972, the characters’ voices were provided by his son Ross Bagdasarian Jr. and the latter\\'s wife Janice Karman in the subsequent incarnations of \"'],\n",
       " [\"Jamie Lee Curtis |  Jamie Lee Curtis (born November 22, 1958) is an American actress and writer. She is the recipient of several accolades, including a British Academy Film Award, two Golden Globe Awards and a star on the Hollywood Walk of Fame in 1998. Curtis made her film acting debut as Laurie Strode in John Carpenter's horror film Halloween (1978), which established her as a scream queen, and she thereafter appeared in a string of horror films, including The Fog, Prom Night, Terror Train (all 1980) and Roadgames (1981). She reprised the role of Laurie in the sequels Halloween II (1981), Halloween H20: 20 Years Later (1998), Halloween: Resurrection (2002), Halloween (2018), and Halloween Kills (2021). Her filmography is largely characterized by independent film that have been box-office successes, with 8 of her lead-actress credits \"]]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backgrounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f9b8e61e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[INST] Refer to the background document and answer the questions:\\n\\nBackground: <xRAG>\\n\\nQuestion: Who was the man behind The Chipmunks?? [/INST] The answer is:',\n",
       " '[INST] Refer to the background document and answer the questions:\\n\\nBackground: <xRAG>\\n\\nQuestion: What star sign is Jamie Lee Curtis?? [/INST] The answer is:']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "93c85eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_embeds = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "437019ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.eval.run_eval import prepare_retrieval_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "46730e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing document embedding with Salesforce/SFR-Embedding-Mistral...\n"
     ]
    }
   ],
   "source": [
    "retrieval_embeds = None\n",
    "if retriever is not None:\n",
    "    # backgrounds List[List[String]]\n",
    "    num_samples = len(backgrounds)\n",
    "    original_orders = []\n",
    "    for idx,background in enumerate(backgrounds):\n",
    "        original_orders.extend(\n",
    "            [idx] * len(background)\n",
    "        )\n",
    "        \n",
    "    backgrounds = [x for y in backgrounds for x in y]\n",
    "    print(f\"Preparing document embedding with {args.retriever_name_or_path}...\")\n",
    "    _retrieval_embeds = prepare_retrieval_embeds(\n",
    "        backgrounds,\n",
    "        retriever,\n",
    "        retriever_tokenizer,\n",
    "    )\n",
    "\n",
    "    retrieval_embeds = [[] for _ in range(num_samples)]\n",
    "    assert len(_retrieval_embeds) == len(original_orders)\n",
    "    for id,embeds in zip(original_orders,_retrieval_embeds):\n",
    "        retrieval_embeds[id].append(embeds)\n",
    "\n",
    "    retriever = retriever.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "53289e41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(retrieval_embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3cedf470",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_prompt_length = tokenizer(prompts,return_length=True).length\n",
    "avg_prompt_length = sum(avg_prompt_length)/len(avg_prompt_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7eb58c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    MistralForCausalLM,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    MixtralForCausalLM,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "234ac6f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brimmann/works/xRAG/.venv/lib/python3.9/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60774f4910ea4783835ade1aa0d71d4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brimmann/works/xRAG/.venv/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the disk and cpu.\n"
     ]
    }
   ],
   "source": [
    " ## load llm\n",
    "config = AutoConfig.from_pretrained(args.model_name_or_path)\n",
    "MODEL_CLASS = eval(config.architectures[0])\n",
    "model = MODEL_CLASS.from_pretrained(\n",
    "    args.model_name_or_path,\n",
    "    torch_dtype = torch.bfloat16,\n",
    "    low_cpu_mem_usage = True,\n",
    "    device_map='auto',\n",
    "    offload_folder=\"./offload\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a3ac0888",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XMistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(32002, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MistralDecoderLayer(\n",
       "        (self_attn): MistralSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): MistralRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm()\n",
       "        (post_attention_layernorm): MistralRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
       "  (projector): Projector(\n",
       "    (projector): Sequential(\n",
       "      (0): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c3d41e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.language_modeling.utils import (\n",
    "    XRAG_TOKEN,\n",
    "    get_retrieval_embeds,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d6c3cc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if retriever is not None:\n",
    "    assert XRAG_TOKEN in tokenizer.get_vocab() \n",
    "    model.set_xrag_token_id(tokenizer.convert_tokens_to_ids(XRAG_TOKEN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3f861d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.eval.run_eval import llm_for_open_generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2cc27581",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                 | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking update\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [01:56, 29.23s/it]                                      \n"
     ]
    }
   ],
   "source": [
    "if args.task_type in ['open_qa','fact_checking']:\n",
    "    generated_results = llm_for_open_generation(\n",
    "        llm = model,\n",
    "        llm_tokenizer = tokenizer,\n",
    "        prompts = prompts,\n",
    "        retrieval_embeds = retrieval_embeds,\n",
    "        batch_size = args.eval_batch_size,\n",
    "        enable_progress_bar= args.enable_progress_bar,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3879ad2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.eval.utils import (\n",
    "    stop_sequences_criteria,\n",
    "    get_substring_match_score,\n",
    "    eval_fact_checking,\n",
    "    eval_truthfulqa,\n",
    "    keyword_extraction_with_tfidf,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "abfc8e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = [x['answer'] for x in test_data]\n",
    "if args.eval_metrics == 'substring_match':\n",
    "    score,score_per_sample = get_substring_match_score(generated_results,answers)\n",
    "elif args.eval_metrics == 'fact_checking_acc':\n",
    "    score,score_per_sample = eval_fact_checking(generated_results,answers)\n",
    "elif args.eval_metrics == 'truthfulqa_f1_rl':\n",
    "    f1,rl,f1_scores,rl_scores = eval_truthfulqa(generated_results,answers)\n",
    "    score = f\"{f1}-{rl}\"\n",
    "    score_per_sample = [(f1_score,rl_score) for f1_score,rl_score in zip(f1_scores,rl_scores)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e3dcc96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict =   {\n",
    "    \"dataset\":args.data,\n",
    "    \"batch_size\":args.eval_batch_size,\n",
    "    \"include_retrieval\":args.use_rag,\n",
    "    \"avg_prompt_length\":avg_prompt_length,\n",
    "    \"model\":args.model_name_or_path,\n",
    "    f\"{args.eval_metrics}\":score,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b34b6c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bc10239f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"dataset\": \"triviaqa\",\n",
      "    \"batch_size\": 4,\n",
      "    \"include_retrieval\": true,\n",
      "    \"avg_prompt_length\": 43.5,\n",
      "    \"model\": \"Hannibal046/xrag-7b\",\n",
      "    \"substring_match\": 0.0,\n",
      "    \"retriever\": \"Salesforce/SFR-Embedding-Mistral\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "if args.retriever_name_or_path is not None:\n",
    "    result_dict['retriever'] = args.retriever_name_or_path\n",
    "print(json.dumps(result_dict,indent=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
